% Created 2021-03-14 Sun 16:03
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{setspace}

\date{\today}
\title{Entropy in Evolutionary Algorithms}
\author{Lucas Blakeslee\footnote{\texttt{lqblakeslee@gmail.com}},
  Aengus McGuinness\footnote{\texttt{aengus82520@gmail.com}}}
\hypersetup{
 pdfauthor={},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.3)}, 
 pdflang={English}}

\setlength\parindent{24pt}

\begin{document}
\maketitle
%% \tableofcontents


\label{sec:org26f53e0}
\begin{abstract}
\label{sec:orga17da23}

Genetic algorithms (GAs) are an optimization technique inspired by
natural selection. GAs have yielded good results in certain practical
problems, yet there is still more to be understood about their
behavior on a theoretical level. One approach is to look at the
evolutionary process from the point of view of statistical mechanics,
and interpreting jumps in fitness as phase transitions. Toward this
goal we examine the behavior of \emph{entropy} in a GA that optimizes
a simple function.  We find that entropy increases as a new species
diversifies, but its upper bound decreases with most phase
transitions (which correspond to evolutionary steps).
\end{abstract}


%% -\textbf{- mode: org -}-


\section{Motivation}
\label{sec:org16abecd}
Genetic algorithms (GAs) are stochastic search algorithms based on the
process of natural evolution, solving for the 'fittest' solutions to a
problem. GAs are optimization algorithms, designed to maximize or
minimize certain functions. GAs are much more efficient than random or
exhaustive search algorithms (Kinnear, 1994), however, they do not
scale well with complexity (Radcliffe \& Surry, 1995). While specifics
may vary, there are some processes universal to GAs, which are:
population â†’ fitness evaluation â†’ selection â†’ crossover â†’ mutation,
repeat. Each organism in the population has its own 'chromosome,' or
set of characteristics. Here is where the resemblance to biological
evolution begins to weaken. GAs continue to occur either
until some condition for termination has been met, or until a
pre-specified number of iterations has elapsed. \\
Genetic algorithms
have been used in a variety of practical applications, from making
"evolved antennae" for spacecrafts and satellites to the estimation of
heat flux between sea ice and the atmosphere to predictive economic
models. \\
Entropy and evolution have been often explored together, with
research on the subject going as far back as the 1870s, though
SchrÃ¶dinger's 1944 book \emph{What is life?} sparked a wave of modern
interest. Here we wondered how entropy would change over time in a
genetic algorithm, in order to attain a better theoretical grasp of
how they function.

\section{Hypothesis}
\label{sec:org26a3be1}
We hypothesized that entropy would gradually rise as small non-significant
mutations occured, and that occasionally one highly beneficial mutation in
an organism would cause entropy to rapidly decrease as that mutation was
selected for and spreads throughout the population, and that entropy would then
begin to rise again in a cyclical pattern. We hypothesized that if we're considering
entropy to be the number of possible states an organism can be in, the
upper bounds of the entropy would never reach levels they were previously at.


\section{Genetic algorithms in Brief}

In brief Genetic Algorithms attempt to find the maximum value of a
function (called a \emph{fitness} function), such as those shown in
Figure~\ref{fig:fit-func_pid3095791}

They do so by having a population of candidate values that take steps
through the domain of that function.  The steps have a random
component, but the randomness is directed by a principle akin to
natural selection: when a population member has a high fitness value,
it is more likely to survive and breed new population members that
share some of its characteristics.

Figure~\ref{fig:gen-info_pid3095791} shows an example of such an
evolution.

Some terminology is used in talking about GA evolution:

\begin{description}
\item[Population] The collection of values currently being considered
  as possible solutions to the problem.
\item[Fitness] A function for which we are trying to find the maximum.
\item[Fitness Landscape] A visualization of the fitness function which
  shows the peaks in fitness, as well as the pitfalls in searching for
  the peaks.
\item[Reproduction] The breeding of two parent individuals into two children. Organisms are
floats, and we can think of them as sequences of 32 bits in IEEE 754 format.
\item[Mutation] At the end of each generation the organisms undergo "mate drift" mutation, 
wherein their offspring shift a small amount in one direction, with a low possibility 
of large drifts occuring.
\end{description}



\section{Approach}
\label{sec:org8c588e5}

To explore how the entropy of the GA population behaves we crafted an
optimization problem that would bring out some well defined steps in
fitness improvement, and also allow a period of stasis between
successive fitness peaks.

\subsection{Fitness functions}

The function we settled on is shown in
Figure~\ref{fig:fit-func_pid3095791}: peaks are separated by valleys,
and it can take the evolutionary process a while for a mutation to
transport a population member across the valley and into the higher
fitness area.

\begin{figure}
  \centering
  \resizebox{\linewidth}{!}{%
  \includegraphics{fit-func_pid3815286.pdf}
  \includegraphics{fit-func_pid3728854.pdf}
  }
  \caption{Two examples of fitness functions: one has a $\sin()$ wave
    riding on a gaussian, the other has a flat-top wave riding on it.
  Most of our results are based on the ``flat-top'' function.}
  \label{fig:fit-func_pid3095791}
\end{figure}

In addition to these two fitness functions, we have also used a
polynomial fitness function, where the GA searched for the highest
peak in a negative degree polynomial. However, we found that the 
polynomial fitness function was not rich enough to yield interesting results.



\subsection{Shannon Entropy}

At each step in the evolution we calculate the \emph{Shannon Entropy},
a measure of the information content by applying Shannon's formula

$$H(X) = -\sum p(X)\log p(X)$$

where $p(x)$ is the occupancy of each state in the population, and we
see how it behaves when there is a step in fitness.

Shannon Entropy is widely discussed elsewhere, but we can give a
simple example here: 




\section{Methods}

Originally, we constructed a genetic algorithm to find the maxima of a
polynomial using the IEEE 754 floating point standard to represent the
chromosomes of an inidividual. The polynomial was structured as: ax\textsuperscript{7}
+ ax\textsuperscript{6} \ldots + ax\textsuperscript{2} + ax + a. Later we
decided to have the software find the maxima of other types of functions,
in our case a $\sin$ wave riding on top of a gaussian. $ P(x) = \frac{1}{{\sigma \sqrt {2\pi } }} e^{{{ - \left( {x - \mu} \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}} $

The population of 10,000 underwent the standard evalutation, selection,
crossover, and mutation of a  GA, with information about the population
being collected at the end of each iteration. Diversity was measured using
the hamming distance between the binary strings of two organisms, and the
Shannon entropy of the population was calculated at the end of each generation.
Additionally, the elite and average fitnesses of the population were tracked.

\subsection{Mate Drift vs Bitflip}
Originally we represented individuals with a 32 bit binary string, and occasionally 
flipped a random bit as mutation. Depending on whether a bit in the sign, the mantissa, 
or the exponent was flipped, the impact the mutation had could vary drastically,
much like how in biological evolution many mutations are silent or have little impact,
 however occasionally  they can lead to very rapid change. Using this technique,
 we had a very clear understanding of what mutation actually is. However, we found 
that this led to some unprecedented issues regarding the numerical limitations
of a 32 bit string. For example, in the string $0 01111111 11111111111111111111111$,
the probability of the first bits flipping to a 1and remaining advantageous is incredibly
low, as the majority of the other bits would have to flip to a 0.

\section{Results}
\label{sec:orged0917a}
We found that rapid drops in entropy accompanied increases of fitness, and
additionally that the upper bound of the entropy never reached previous levels.
Fitness changed in dramatic occasional jumps.

\begin{figure}[h]
  \centering
  \resizebox{\linewidth}{!}{%
    \includegraphics{GA-gen-info_pid3095791.out.pdf}
  }
  \caption{An example of evolution.  The top panel shows fitness as a
    function of generation for a ``$\sin()$ on top of gaussian''
    fitness landscape (see Figure~\ref{fig:fit-func_pid3095791}).  The
    middle panel shows entropy, where you can see that the end point
    of the evolution has a lower entropy than earlier phases.}
  \label{fig:gen-info_pid3095791}
\end{figure}

\begin{figure}[h!]
  \centering
  \resizebox{0.6\linewidth}{!}{%
    \includegraphics{GA-gen-info_pid3027915.out.pdf}
  }
  \caption{Another example of evolution that shows more clearly how
    the entropy reaches lower pleateaus with increased fitness..}
  \label{fig:gen-info_pid3027915}
\end{figure}




\section{Discussion \& Conclusions}
\label{sec:org7999995}
Entropy rapidly falls as the average fitness makes large steps. This is because as the x-position value nears the local maxima the preceding values fall out of the population as the fittest individuals progress to the next generation. We observed occasional dramatic jumps in fitness as opposed to a linear development. This is corresponds with punctuated equilibrium, a theory in evolutionary biology that evolutionary development is marked by isolated episodes of rapid speciation between long periods of little or no change (Punctuated Equilibrium, 2020).
\begin{figure}[h]
	\begin{center}
	\includegraphics[scale=0.25]{Punctuated_Equilibrium.pdf}
	\caption{
		\small Ian Alexander, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons
	}
\end{center}
\end{figure}

\subsection{What does this mean?}
\label{secðŸ˜®rgf7b36ed}
In Figure 3 we see how when the average, elite, and max fitness take a step forward the entropy comes tumbling down. This means that as a species evolves and better adapts to its environments and beats out the competition entropy decreases. This does not violate the second law of thermodynamics because that only applies to an adiabatic system.

\subsection{Limitations to genetic algorithms}
\label{sec:org148bf83}
There are some limitations to genetic algorithms. For example, one must
have a very clear understanding of the problem, constraints, the data
structure, etc. Additionally, genetic algorithms do not scale well with
complexity -- problems with large numbers of elements often become exponentially
more difficult to compute. There is also the difficulty of ensuring the
algorithm doesn't get stuck on a local maxima rather than the global one.

\subsection{Future work}
\label{sec:org0f04af1}
Going forward, it could be interesting to optimize a genetic algorithm from the standpoint of its entropy.

\section{References}
\label{sec:org9dc046e}
\textbf{need to be alphebatized by last name of the first author}\\
\doublespacing
Kinnear, K. E. (1994). In K. E. Kinnear (Ed.), \emph{Advances in Genetic Programming} (pp. 3-17). Cambridge: MIT Press.

Radcliffe N.J., Surry P.D. (1995) Fundamental limitations on search algorithms: Evolutionary computing in perspective.
In: van Leeuwen J. (eds) Computer Science Today. Lecture Notes in Computer Science, vol 1000. Springer, Berlin, Heidelberg.
$https://doi.org/10.1007/BFb0015249$

Punctuated Equilibrium. (2020). In Oxford Online Dictionary. Retrieved from 
$https://www.lexico.com/definition/punctuated_equilibrium$
(Punctuated Equilibrium, 2020)
\end{document}
